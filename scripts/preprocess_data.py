#!/usr/bin/env python3
"""
Fisher-Flow MOLS ë°ì´í„° ì „ì²˜ë¦¬ ë° ìºì‹±
RTX 3080 ë“€ì–¼ GPU í™˜ê²½ ìµœì í™”
"""

import torch
import torch.nn.functional as F
import numpy as np
import json
import argparse
from pathlib import Path
from tqdm import tqdm
import wandb
from typing import Dict, List, Tuple

from src.datasets.latin_dataset import LatinSquareDataset, FisherGeometry


def create_splits(data_dir: Path, target_orders: List[int] = [8, 9]) -> Dict[str, Dict]:
    """
    Fisher-Flow í•™ìŠµìš© ë°ì´í„° split ìƒì„±
    
    Split ì „ëµ:
    - Train: n â‰¤ 7 (3,4,5,7)  
    - Val: n = 8
    - Test: n = 9
    """
    splits = {
        'train': {'orders': [3, 4, 5, 7], 'samples': []},
        'val': {'orders': [8], 'samples': []},
        'test': {'orders': [9], 'samples': []}
    }
    
    # ê° splitë³„ë¡œ ë°ì´í„° ë¡œë“œ
    for split_name, split_info in splits.items():
        print(f"\n=== {split_name.upper()} Split ìƒì„± ===")
        
        dataset = LatinSquareDataset(
            data_dir=data_dir,
            orders=split_info['orders'],
            transform_to_sphere=True,
            epsilon=1e-4,
            augment_rotations=False,  # ìºì‹±ì‹œì—ëŠ” augmentation ë¹„í™œì„±í™”
            augment_relabeling=False
        )
        
        split_info['dataset'] = dataset
        split_info['num_samples'] = len(dataset)
        
        print(f"{split_name}: {len(dataset)} samples from orders {split_info['orders']}")
    
    return splits


def cache_tensors(splits: Dict, cache_dir: Path, batch_size: int = 32):
    """
    Fisher-Rao ë³€í™˜ëœ í…ì„œë“¤ì„ .pt íŒŒì¼ë¡œ ìºì‹±
    GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬
    """
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # GPU ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if torch.cuda.is_available():
        print(f"GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    for split_name, split_info in splits.items():
        print(f"\n=== {split_name.upper()} ìºì‹± ì¤‘ ===")
        
        dataset = split_info['dataset']
        split_cache = []
        
        # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        for i in tqdm(range(0, len(dataset), batch_size), desc=f"Caching {split_name}"):
            batch_data = []
            
            for j in range(i, min(i + batch_size, len(dataset))):
                sample = dataset[j]
                
                # GPUë¡œ ì´ë™ í›„ ì²˜ë¦¬
                if 'sphere_points' in sample:
                    sample['sphere_points'] = sample['sphere_points'].to(device)
                if 'one_hot' in sample:
                    sample['one_hot'] = sample['one_hot'].to(device)
                if 'square' in sample:
                    sample['square'] = sample['square'].to(device)
                
                batch_data.append(sample)
            
            # CPUë¡œ ì´ë™ í›„ ì €ì¥
            for sample in batch_data:
                for key in ['sphere_points', 'one_hot', 'square']:
                    if key in sample:
                        sample[key] = sample[key].cpu()
                split_cache.append(sample)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # ìºì‹œ íŒŒì¼ ì €ì¥
        cache_file = cache_dir / f"{split_name}_cache.pt"
        torch.save({
            'samples': split_cache,
            'orders': split_info['orders'],
            'num_samples': len(split_cache),
            'epsilon': 1e-4,
            'transform_info': 'Fisher-Rao mapping to S^d_+'
        }, cache_file)
        
        print(f"{split_name} ìºì‹œ ì €ì¥: {cache_file}")
        print(f"íŒŒì¼ í¬ê¸°: {cache_file.stat().st_size / 1e6:.1f} MB")


def validate_cache(cache_dir: Path):
    """ìºì‹œëœ ë°ì´í„° ê²€ì¦"""
    print("\n=== ìºì‹œ ê²€ì¦ ===")
    
    geometry = FisherGeometry()
    
    for split in ['train', 'val', 'test']:
        cache_file = cache_dir / f"{split}_cache.pt"
        
        if not cache_file.exists():
            print(f"Warning: {cache_file} not found")
            continue
            
        data = torch.load(cache_file)
        samples = data['samples']
        
        print(f"\n{split.upper()}:")
        print(f"  Samples: {len(samples)}")
        print(f"  Orders: {data['orders']}")
        
        # ì²« ë²ˆì§¸ ìƒ˜í”Œ ê²€ì¦
        if samples:
            sample = samples[0]
            
            if 'sphere_points' in sample:
                sphere_pts = sample['sphere_points']
                print(f"  Sphere shape: {sphere_pts.shape}")
                print(f"  Sphere norm: {torch.norm(sphere_pts[0,0]):.4f}")
                
                # Round-trip í…ŒìŠ¤íŠ¸
                flat_sphere = sphere_pts.view(-1, sphere_pts.shape[-1])
                recovered = geometry.from_sphere(flat_sphere)
                one_hot_flat = sample['one_hot'].view(-1, sample['one_hot'].shape[-1])
                error = torch.max(torch.abs(recovered - one_hot_flat))
                print(f"  Round-trip error: {error:.6f}")


def create_wandb_artifact(cache_dir: Path, project_name: str = "fisher-flow-mols"):
    """W&B Artifact ìƒì„±"""
    print("\n=== W&B Artifact ìƒì„± ===")
    
    # W&B ì´ˆê¸°í™”
    wandb.init(project=project_name, job_type="data-preprocessing")
    
    # Artifact ìƒì„±
    artifact = wandb.Artifact(
        name="mols_fisher_cache",
        type="dataset",
        description="Fisher-Flow MOLS dataset with Fisher-Rao transformations",
        metadata={
            "orders": [3, 4, 5, 7, 8, 9],
            "target_orders": [8, 9],
            "epsilon": 1e-4,
            "transform": "Fisher-Rao mapping to S^d_+",
            "split_strategy": "train(â‰¤7), val(8), test(9)"
        }
    )
    
    # ìºì‹œ íŒŒì¼ë“¤ ì¶”ê°€
    for cache_file in cache_dir.glob("*_cache.pt"):
        artifact.add_file(str(cache_file), name=cache_file.name)
    
    # Artifact ë¡œê·¸
    wandb.log_artifact(artifact)
    wandb.finish()
    
    print("W&B Artifact ì €ì¥ ì™„ë£Œ!")


def get_data_statistics(cache_dir: Path):
    """ë°ì´í„°ì…‹ í†µê³„ ì¶œë ¥"""
    print("\n=== ë°ì´í„°ì…‹ í†µê³„ ===")
    
    total_samples = 0
    for split in ['train', 'val', 'test']:
        cache_file = cache_dir / f"{split}_cache.pt"
        if cache_file.exists():
            data = torch.load(cache_file)
            samples = len(data['samples'])
            total_samples += samples
            orders = data['orders']
            
            print(f"{split:>5}: {samples:>6} samples (orders: {orders})")
    
    print(f"{'Total':>5}: {total_samples:>6} samples")
    
    # GPU ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ì¶”ì •
    if total_samples > 0:
        # ì˜ˆì‹œ: 9x9 square â†’ 9x9x9 one-hot â†’ 729 float32 = ~3KB/sample
        estimated_memory = total_samples * 3 * 1024  # bytes
        print(f"\nì¶”ì • GPU ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰: {estimated_memory / 1e6:.1f} MB")
        print(f"RTX 3080 (10GB) ë°°ì¹˜ í¬ê¸° ê¶Œì¥: 32-64")


def main():
    parser = argparse.ArgumentParser(description="Fisher-Flow MOLS ë°ì´í„° ì „ì²˜ë¦¬")
    parser.add_argument("--data-dir", type=str, default="data/raw", 
                       help="ì›ë³¸ MOLS ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--cache-dir", type=str, default="data/cache",
                       help="ìºì‹œ ì €ì¥ ë””ë ‰í† ë¦¬") 
    parser.add_argument("--batch-size", type=int, default=32,
                       help="ìºì‹± ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--wandb", action="store_true",
                       help="W&B Artifact ìƒì„±")
    parser.add_argument("--validate-only", action="store_true",
                       help="ìºì‹œ ê²€ì¦ë§Œ ì‹¤í–‰")
    
    args = parser.parse_args()
    
    data_dir = Path(args.data_dir)
    cache_dir = Path(args.cache_dir)
    
    if args.validate_only:
        validate_cache(cache_dir)
        get_data_statistics(cache_dir)
        return
    
    print("ğŸ”„ Fisher-Flow MOLS ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
    print(f"ì›ë³¸ ë°ì´í„°: {data_dir}")
    print(f"ìºì‹œ ì €ì¥: {cache_dir}")
    
    # 1. ë°ì´í„° split ìƒì„±
    splits = create_splits(data_dir)
    
    # 2. í…ì„œ ìºì‹±
    cache_tensors(splits, cache_dir, args.batch_size)
    
    # 3. ìºì‹œ ê²€ì¦
    validate_cache(cache_dir)
    
    # 4. í†µê³„ ì¶œë ¥
    get_data_statistics(cache_dir)
    
    # 5. W&B Artifact (ì„ íƒì‚¬í•­)
    if args.wandb:
        create_wandb_artifact(cache_dir)
    
    print("\nâœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ë‹¤ìŒ ë‹¨ê³„: python cli.py train --data-cache {cache_dir}")


if __name__ == "__main__":
    main() 